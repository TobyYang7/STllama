
  0%|                                                                                                                                    | 0/10449 [00:00<?, ?it/s]
pre: tensor([ 0.2532, -0.4016, -0.2786, -0.6304,  0.2264, -0.2347,  0.3013, -0.0545,
         1.1426, -0.5845, -0.9575, -0.3633], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>)
true: tensor([118.0000,  63.5000,  43.7500,  32.5000,  26.1250,  21.7500,  19.5000,
         16.3750,  16.7500,  56.0000, 247.0000, 135.0000], device='cuda:0',
       dtype=torch.bfloat16)
llm: 6.97625732421875
reg: 260.067626953125
pre: tensor([-0.0775, -0.5815,  0.4209, -0.4797, -0.0870,  0.1760,  0.5688, -0.6465,
         0.7646, -0.3882,  0.3774,  0.4734], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>)
true: tensor([ 51., 156., 107.,  43.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],
       device='cuda:0', dtype=torch.bfloat16)
llm: 7.2217698097229
reg: 40.892913818359375
pre: tensor([-0.0011, -0.6353, -0.3518,  0.1749, -0.1290,  0.3154,  0.2585, -0.3228,
         0.9019,  0.7236, -0.1158,  0.1942], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>)
true: tensor([29., 39., 35., 31., 27., 26., 32., 41., 39., 33., 29., 29.],
       device='cuda:0', dtype=torch.bfloat16)
llm: 6.777491569519043
reg: 1343.3359375
pre: tensor([ 0.0461, -0.1317, -0.8179, -0.4155, -0.0039,  0.0803, -0.0938, -0.2998,
         1.2197, -0.5249, -0.2356, -0.6182], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>)
true: tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,  23.2500,  11.3125,
          8.3750,  11.5000, 704.0000,  93.5000,  56.7500], device='cuda:0',
       dtype=torch.bfloat16)
llm: 6.854537010192871
reg: 127.65310668945312

  0%|                                                                                                                         | 1/10449 [00:06<19:11:09,  6.61s/it]
pre: tensor([-0.2673, -0.6953,  0.1115, -0.0427,  0.3496,  1.0264, -0.5112, -0.4731,
         0.4790,  0.3950,  0.0159, -0.7866], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>)
true: tensor([ 66.0000, 206.0000, 258.0000, 171.0000, 199.0000,  99.0000, 382.0000,
         44.2500,  59.2500,  50.0000,  39.2500,  37.5000], device='cuda:0',
       dtype=torch.bfloat16)
llm: 6.668460369110107
reg: 4037.732177734375
pre: tensor([-0.0938, -0.6855,  0.2107, -0.2346,  0.1882,  0.6543, -0.1995,  0.2429,
         1.0586, -0.3567, -0.2261,  0.3132], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>)
true: tensor([146., 274., 187., 137., 102.,  77., 102., 164., 152., 144., 135., 133.],
       device='cuda:0', dtype=torch.bfloat16)
llm: 6.808465480804443
reg: 136.14170837402344
pre: tensor([ 0.0935,  0.1827, -0.4043, -0.7246,  0.2583,  0.2676,  0.0370, -0.3196,
         1.2031,  0.1192, -0.3984,  0.0500], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>)
true: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',
       dtype=torch.bfloat16)
llm: 6.855809211730957
reg: 72.3733139038086
pre: tensor([ 0.1807, -0.6265,  0.5283, -0.8384,  0.0255,  0.0806,  0.1154, -0.0356,
         0.9561, -0.2686, -0.3538, -0.3562], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>)
true: tensor([  6.6250,   3.4531,   3.3750,   1.3906,   3.0625,   3.2344,   1.3125,
          1.3516,   1.3984,   2.5625, 290.0000,  42.2500], device='cuda:0',
       dtype=torch.bfloat16)
llm: 6.955572605133057
reg: 15.099328994750977

  0%|                                                                                                                         | 2/10449 [00:12<18:02:30,  6.22s/it]
pre: tensor([ 0.0530, -0.4966,  0.3196, -0.1268,  0.2043,  0.3843, -0.7334, -0.0922,
         1.3291,  0.1191, -0.1044, -0.1664], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>)
true: tensor([  2.2656,   3.5625,   4.4688,   4.3750,   2.5781,   3.1406, 356.0000,
        205.0000,  52.5000,  39.2500,  23.5000,  28.1250], device='cuda:0',
       dtype=torch.bfloat16)
llm: 6.7985100746154785
reg: 233.2608642578125
pre: tensor([ 0.0556, -0.4900,  0.0807, -0.0514,  0.0178,  0.4082,  0.1447, -0.3645,
         0.6294, -0.3701,  0.1122,  0.5840], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>)
true: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',
       dtype=torch.bfloat16)
llm: 6.430959701538086
reg: 532.4247436523438
pre: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)
true: tensor([27., 28., 28., 23., 20., 20., 20., 21., 23., 24., 24., 24.],
       device='cuda:0', dtype=torch.bfloat16)
llm: nan
reg: nan
pre: tensor([-0.2900, -0.8101, -0.0186,  0.5044,  0.1409,  0.5903, -0.1517, -0.0166,
         0.6880, -0.5093,  0.0216,  0.3406], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>)
true: tensor([36., 36., 44., 52., 60., 60., 59., 57., 57., 70., 58., 67.],
       device='cuda:0', dtype=torch.bfloat16)
llm: 6.6850199699401855
reg: 152.92611694335938

  0%|                                                                                                                         | 3/10449 [00:18<17:25:01,  6.00s/it]
pre: tensor([ 0.4229, -0.5503,  0.2260, -0.4553,  0.0906,  0.0522, -0.1803, -0.4736,
         1.0303,  0.0616,  0.0707,  0.4092], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>)
true: tensor([ 16.,  71., 121.,  66.,  28.,  17.,  18.,  17.,  17.,  26.,  20.,  18.],
       device='cuda:0', dtype=torch.bfloat16)
llm: 6.5540642738342285
reg: 542.5598754882812
pre: tensor([ 0.0136, -0.5576, -0.5483, -0.7603,  0.3047, -0.1831, -0.0924, -0.3545,
         1.4209, -0.0018,  0.0910,  0.3379], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>)
true: tensor([158.,  63.,  35.,  24.,  18.,  17.,  16.,  15.,  14.,  13.,  12.,  11.],
       device='cuda:0', dtype=torch.bfloat16)
llm: 6.6625590324401855
reg: 193.96878051757812
pre: tensor([ 0.1217, -0.5791, -0.0581, -0.3489,  0.2561,  0.4221, -0.6934, -0.4243,
         1.6533, -0.5972, -0.7109, -0.7476], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>)
true: tensor([166., 145., 125., 105.,  86.,  66.,  47.,  39.,  48.,  48.,  98., 138.],
       device='cuda:0', dtype=torch.bfloat16)
llm: 6.875661849975586
reg: 686.2496948242188
pre: tensor([-0.0256, -0.6943,  0.2603, -0.3230,  0.2861,  0.5176, -0.5698,  0.1742,
         1.1660, -0.0338, -0.1686,  0.1564], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>)
true: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',
       dtype=torch.bfloat16)
llm: 6.775287628173828
reg: 891.70947265625
{'loss': 585.3389, 'learning_rate': 1.2738853503184714e-05, 'epoch': 0.0}
pre: tensor([-0.0023, -0.8208,  0.3213, -0.2178, -0.0302,  0.2065,  0.2338, -0.3198,
         1.3320, -0.1626, -0.0218,  0.1025], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>)
true: tensor([28., 27., 27., 26., 26., 28., 29., 31., 32., 33., 35., 33.],
       device='cuda:0', dtype=torch.bfloat16)
llm: nan

  0%|                                                                                                                         | 4/10449 [00:24<17:08:44,  5.91s/it]
pre: tensor([ 0.2151, -0.6133,  0.0738,  0.0621,  0.3049,  0.5806, -0.4343, -0.0605,
         1.4990,  0.1171, -0.3569, -0.5151], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>)
true: tensor([ 89.,  59.,  43.,  83., 215., 272., 228., 185., 151., 122.,  95.,  71.],
       device='cuda:0', dtype=torch.bfloat16)
llm: 6.602352619171143
reg: 2146.344970703125
pre: tensor([ 0.0521, -0.3804, -0.0847, -0.0630,  0.6138, -0.0409, -0.7656, -0.4104,
         0.6885,  0.4390,  0.5693, -0.0974], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>)
true: tensor([ 44.,  44.,  41.,  39.,  38.,  38.,  38.,  55.,  78., 101., 118., 102.],
       device='cuda:0', dtype=torch.bfloat16)
llm: nan
reg: nan
pre: tensor([-0.2386, -0.6074, -0.2147, -0.4573,  0.1093,  0.5444, -0.1803, -0.8135,
         0.9028, -0.2045, -0.3972,  0.5479], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>)
true: tensor([45.2500, 45.2500, 45.2500, 45.2500, 36.2500, 43.2500, 41.0000, 42.0000,
        40.0000, 43.2500, 33.5000, 42.2500], device='cuda:0',
       dtype=torch.bfloat16)
llm: 6.581028938293457
reg: 362.49810791015625
{'loss': 1168.7434, 'learning_rate': 1.2738853503184714e-05, 'epoch': 0.0}
pre: tensor([-0.3835, -0.7720, -0.2915, -0.5161,  0.5532,  0.4421, -0.0963, -0.4006,
         1.3496, -0.4585, -0.4246,  0.7959], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>)
true: tensor([ 35.,  26.,  23., 124., 137.,  89.,  58.,  38., 256., 426., 250., 201.],
       device='cuda:0', dtype=torch.bfloat16)
llm: 6.7055344581604

  0%|                                                                                                                         | 5/10449 [00:30<17:11:26,  5.93s/it]
pre: tensor([-0.0245, -0.2048,  0.1483, -0.4900,  0.3770,  0.5845,  0.1020, -0.9243,
         1.0986, -0.0642, -0.0789, -0.0543], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>)
true: tensor([  4.8125,   3.4844,   5.0625,   6.1875,   4.6562,   2.8594,   2.6406,
          2.5625,   3.1719,   1.3438,   2.0000, 330.0000], device='cuda:0',
       dtype=torch.bfloat16)
llm: nan
reg: nan
pre: tensor([-0.0043,  0.1777, -0.1306, -0.8989,  0.3345,  0.2007, -0.0717, -0.6870,
         1.0654, -0.2717, -0.1851, -0.1517], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>)
true: tensor([11.5000, 15.1875, 14.1250, 14.4375,  7.4688,  4.1875,  9.8750, 34.7500,
        27.0000, 22.2500, 20.1250, 13.0625], device='cuda:0',
       dtype=torch.bfloat16)
llm: 6.779374122619629
reg: 10.084298133850098
pre: tensor([ 0.2991, -0.8486,  0.1135, -0.1167,  0.5791,  0.4023, -1.0547, -0.2825,
         1.6494, -0.1633, -1.4443, -0.3865], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>)
true: tensor([190., 158., 139., 178., 147., 111.,  84.,  66.,  56.,  50.,  45.,  41.],
       device='cuda:0', dtype=torch.bfloat16)
llm: 6.573923110961914
reg: 1591.6767578125
{'loss': 601.1536, 'learning_rate': 1.2738853503184714e-05, 'epoch': 0.0}
pre: tensor([-0.1490, -0.0695, -0.4009, -0.1112,  0.3186,  0.6372,  0.1439, -0.5337,
         1.1885, -0.0714, -0.3242, -0.0688], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>)
true: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',
       dtype=torch.bfloat16)
llm: 6.771895408630371

  0%|                                                                                                                         | 6/10449 [00:36<17:18:47,  5.97s/it]Traceback (most recent call last):
  File "urbangpt/train/train_mem.py", line 29, in <module>
    train()
  File "/root/autodl-tmp/STllama/urbangpt/train/train_st.py", line 821, in train
    trainer.train()
  File "/root/miniconda3/lib/python3.8/site-packages/transformers/trainer.py", line 1591, in train
    return inner_training_loop(
  File "/root/miniconda3/lib/python3.8/site-packages/transformers/trainer.py", line 1892, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/root/miniconda3/lib/python3.8/site-packages/transformers/trainer.py", line 2776, in training_step
    loss = self.compute_loss(model, inputs)
  File "/root/miniconda3/lib/python3.8/site-packages/transformers/trainer.py", line 2801, in compute_loss
    outputs = model(**inputs)
  File "/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.8/site-packages/accelerate/utils/operations.py", line 822, in forward
    return model_forward(*args, **kwargs)
  File "/root/miniconda3/lib/python3.8/site-packages/accelerate/utils/operations.py", line 810, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/root/miniconda3/lib/python3.8/site-packages/torch/amp/autocast_mode.py", line 14, in decorate_autocast
    return func(*args, **kwargs)
  File "/root/autodl-tmp/STllama/urbangpt/model/STLlama.py", line 380, in forward
    st_pre_embs1 = hidden_states[:,
KeyboardInterrupt